{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "\n",
    "from timm.utils import accuracy, AverageMeter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import build_model\n",
    "from models.swin_transformer import SwinTransformer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp\n",
    "\n",
    "import torchvision \n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.data import create_transform\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "lr = 0.0001\n",
    "weight_decay = 0.000001\n",
    "t_total = 10000\n",
    "eval_every = 20\n",
    "\n",
    "pt_path = \"./img\"\n",
    "tr_path = \"./img\"\n",
    "ts_path = \"./img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.normal_()\n",
    "        self._commitment_cost = commitment_cost\n",
    "        \n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "        \n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "            \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self._epsilon)\n",
    "                / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        loss = self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Straight Through Estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
    "        super(Residual, self).__init__()\n",
    "        self._block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=num_residual_hiddens,\n",
    "                      kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=num_residual_hiddens,\n",
    "                      out_channels=num_hiddens,\n",
    "                      kernel_size=1, stride=1, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self._block(x)\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self._num_residual_layers = num_residual_layers\n",
    "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
    "                             for _ in range(self._num_residual_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self._num_residual_layers):\n",
    "            x = self._layers[i](x)\n",
    "        return F.relu(x)\n",
    "    \n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens//2,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//2,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "        self._conv_3 = nn.Conv2d(in_channels=num_hiddens,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1, padding=1)\n",
    "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
    "                                             num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._conv_1(inputs)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self._conv_2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self._conv_3(x)\n",
    "        return self._residual_stack(x)\n",
    "    \n",
    "    \n",
    "class Mid_Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(Mid_Decoder, self).__init__()\n",
    "        \n",
    "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3, \n",
    "                                 stride=1, padding=1)\n",
    "        \n",
    "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
    "                                             num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)\n",
    "        \n",
    "        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, \n",
    "                                                out_channels=num_hiddens,\n",
    "                                                kernel_size=4, \n",
    "                                                stride=2, padding=1)\n",
    "        \n",
    "        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens, \n",
    "                                                out_channels=num_hiddens,\n",
    "                                                kernel_size=4, \n",
    "                                                stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._conv_1(inputs)\n",
    "        \n",
    "        x = self._residual_stack(x)\n",
    "        \n",
    "        x = self._conv_trans_1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "        #return self._conv_trans_2(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3, \n",
    "                                 stride=1, padding=1)\n",
    "        \n",
    "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
    "                                             num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)\n",
    "        \n",
    "        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, \n",
    "                                                out_channels=num_hiddens//2,\n",
    "                                                kernel_size=4, \n",
    "                                                stride=2, padding=1)\n",
    "        \n",
    "        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2, \n",
    "                                                out_channels=3,\n",
    "                                                kernel_size=4, \n",
    "                                                stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._conv_1(inputs)\n",
    "        \n",
    "        x = self._residual_stack(x)\n",
    "        \n",
    "        x = self._conv_trans_1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return self._conv_trans_2(x)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, \n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0.99):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        #self._encoder = Encoder(3, num_hiddens,num_residual_layers, num_residual_hiddens)\n",
    "        self._encoder = build_model()\n",
    "        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens, \n",
    "                                      out_channels=embedding_dim,\n",
    "                                      kernel_size=1, \n",
    "                                      stride=1)\n",
    "\n",
    "        self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "                                              commitment_cost, decay)\n",
    "\n",
    "        self._mid_decoder = Mid_Decoder(embedding_dim,\n",
    "                                num_hiddens, \n",
    "                                num_residual_layers, \n",
    "                                num_hiddens)\n",
    "        \n",
    "        self._decoder = Decoder(num_hiddens,\n",
    "                                num_hiddens, \n",
    "                                num_residual_layers, \n",
    "                                num_residual_hiddens)\n",
    "\n",
    "    def forward(self, x, pretrain = False):\n",
    "        x = self._encoder(x,pretrain = pretrain)\n",
    "        x = x.permute(0,1,2)\n",
    "        x = x.view(-1,768,14,14)\n",
    "\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(x)\n",
    "        quantized= self._mid_decoder(quantized)\n",
    "        x_recon = self._decoder(quantized)\n",
    "\n",
    "        return loss, x_recon, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transform():\n",
    "\n",
    "    if True:\n",
    "        # this should always dispatch to transforms_imagenet_train\n",
    "        transform = create_transform(\n",
    "            input_size=224,\n",
    "            is_training=True,\n",
    "            color_jitter=0.4,\n",
    "            auto_augment='rand-m9-mstd0.5-inc1',\n",
    "            interpolation=InterpolationMode.BICUBIC,\n",
    "            re_prob=0.25,\n",
    "            re_mode='pixel',\n",
    "            re_count=1\n",
    "        )\n",
    "\n",
    "        \n",
    "        return transform\n",
    "    \n",
    "def get_loader(pt_path, tr_path, ts_path, pt_batch, tr_batch, ts_batch):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop((224, 224), scale=(0.05, 1.0)), # center crop으로 변경 필요\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    pretrain_transform = build_transform()\n",
    "    \n",
    "    pretrain_dataset = torchvision.datasets.ImageFolder(pt_path,transform = pretrain_transform)\n",
    "    train_dataset = torchvision.datasets.ImageFolder(tr_path,transform = transform_train)\n",
    "    test_dataset = torchvision.datasets.ImageFolder(ts_path,transform = transform_test)\n",
    "\n",
    "    pretrain_train_loader = DataLoader(pretrain_dataset,\n",
    "                              batch_size=pt_batch,\n",
    "                              num_workers=8,\n",
    "                              pin_memory=True,\n",
    "                            shuffle=True\n",
    "                             )\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=tr_batch,\n",
    "                              num_workers=8,\n",
    "                              pin_memory=True,\n",
    "                              shuffle=True\n",
    "                             )\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                              batch_size=ts_batch,\n",
    "                              num_workers=8,\n",
    "                              pin_memory=True\n",
    "                             )\n",
    "    return pretrain_train_loader, train_loader, test_loader\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    model = SwinTransformer(img_size=224,\n",
    "                            patch_size=2,\n",
    "                            in_chans=3,\n",
    "                            num_classes=1,\n",
    "                            embed_dim=96,\n",
    "                            depths=[2, 2, 6, 2],\n",
    "                            num_heads=[3, 6, 12, 24],\n",
    "                            window_size=7,\n",
    "                            mlp_ratio=4,\n",
    "                            qkv_bias=True,\n",
    "                            qk_scale=None,\n",
    "                            drop_rate=0.0,\n",
    "                            drop_path_rate=0.1,\n",
    "                            ape=False,\n",
    "                            patch_norm=True,\n",
    "                            use_checkpoint=False)\n",
    "        #raise NotImplementedError(f\"Unkown model: {model_type}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "              num_embeddings, embedding_dim, \n",
    "              commitment_cost, decay).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(),  eps=1e-08, betas=(0.9, 0.999), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b726bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "i = 0\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "while(True):\n",
    "    i = 0\n",
    "    for data in pretrain_loader:\n",
    "        x,y = data\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            vq_loss, data_recon, perplexity = model(x)\n",
    "            x = F.avg_pool2d(x, kernel_size=2, stride=2, padding=0)\n",
    "            recon_error = F.mse_loss(data_recon, x) / 0.2\n",
    "            loss = recon_error + vq_loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_res_recon_error.append(recon_error.item())\n",
    "        train_res_perplexity.append(perplexity.item())\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('%d iterations' % (i+1))\n",
    "            print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
    "            print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
    "            print()\n",
    "            torch.save(model.state_dict())\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95542f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_param(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_param(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
